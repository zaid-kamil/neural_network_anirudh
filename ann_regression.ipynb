{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5ab50c3-a68d-47ae-a418-a11b11761fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "993834dc-cdcf-4ae0-8775-fbddb9ed36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True) # hide scientfic notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "876bae22-21d7-4c0e-9370-575e880c20e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>KM</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HP</th>\n",
       "      <th>MetColor</th>\n",
       "      <th>CC</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>46986</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>72937</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.0</td>\n",
       "      <td>41711</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1165.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.0</td>\n",
       "      <td>38500</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age     KM  Weight  HP  MetColor      CC  Doors  Price\n",
       "0  23.0  46986  1165.0  90         1  2000.0      3  13500\n",
       "1  23.0  72937  1165.0  90         1  2000.0      3  13750\n",
       "2  24.0  41711  1165.0  90         1  2000.0      3  13950\n",
       "3  26.0  48000  1165.0  90         0  2000.0      3  14950\n",
       "4  30.0  38500  1170.0  90         0  2000.0      3  13750"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('datasets/CarPricesData.pkl') # preprocessed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "615cec55-e82b-4438-90cd-96d2e1c3d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Price')\n",
    "y = df[['Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "721cf314-c2e9-471f-b4ce-0c713ff86967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "featScaler = StandardScaler()\n",
    "targetScaler = StandardScaler()\n",
    "\n",
    "X = featScaler.fit_transform(X)\n",
    "y = targetScaler.fit_transform(y).flatten() # need 1 d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58fa38e4-78c9-4096-80b4-c994c82249e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1148, 7)\n",
      "(1148,)\n",
      "(287, 7)\n",
      "(287,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2b841-13ec-4620-bd25-8dcd33a80a9f",
   "metadata": {},
   "source": [
    "neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21a2cb71-1968-40a0-89cb-99ee6c78c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # nn is going to be in sequence\n",
    "from tensorflow.keras.layers import Dense # a neural layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f9fe8a1-7382-42c1-b56f-a964e25d5f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'glorot_uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'zeros'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Just your regular densely-connected NN layer.\n",
       "\n",
       "`Dense` implements the operation:\n",
       "`output = activation(dot(input, kernel) + bias)`\n",
       "where `activation` is the element-wise activation function\n",
       "passed as the `activation` argument, `kernel` is a weights matrix\n",
       "created by the layer, and `bias` is a bias vector created by the layer\n",
       "(only applicable if `use_bias` is `True`). These are all attributes of\n",
       "`Dense`.\n",
       "\n",
       "Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
       "computes the dot product between the `inputs` and the `kernel` along the\n",
       "last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
       "For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
       "a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
       "of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
       "`batch_size * d0` such sub-tensors).  The output in this case will have\n",
       "shape `(batch_size, d0, units)`.\n",
       "\n",
       "Besides, layer attributes cannot be modified after the layer has been called\n",
       "once (except the `trainable` attribute).\n",
       "When a popular kwarg `input_shape` is passed, then keras will create\n",
       "an input layer to insert before the current layer. This can be treated\n",
       "equivalent to explicitly defining an `InputLayer`.\n",
       "\n",
       "Example:\n",
       "\n",
       ">>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
       ">>> model = tf.keras.models.Sequential()\n",
       ">>> model.add(tf.keras.Input(shape=(16,)))\n",
       ">>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
       ">>> # Now the model will take as input arrays of shape (None, 16)\n",
       ">>> # and output arrays of shape (None, 32).\n",
       ">>> # Note that after the first layer, you don't need to specify\n",
       ">>> # the size of the input anymore:\n",
       ">>> model.add(tf.keras.layers.Dense(32))\n",
       ">>> model.output_shape\n",
       "(None, 32)\n",
       "\n",
       "Args:\n",
       "    units: Positive integer, dimensionality of the output space.\n",
       "    activation: Activation function to use.\n",
       "        If you don't specify anything, no activation is applied\n",
       "        (ie. \"linear\" activation: `a(x) = x`).\n",
       "    use_bias: Boolean, whether the layer uses a bias vector.\n",
       "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
       "    bias_initializer: Initializer for the bias vector.\n",
       "    kernel_regularizer: Regularizer function applied to\n",
       "        the `kernel` weights matrix.\n",
       "    bias_regularizer: Regularizer function applied to the bias vector.\n",
       "    activity_regularizer: Regularizer function applied to\n",
       "        the output of the layer (its \"activation\").\n",
       "    kernel_constraint: Constraint function applied to\n",
       "        the `kernel` weights matrix.\n",
       "    bias_constraint: Constraint function applied to the bias vector.\n",
       "\n",
       "Input shape:\n",
       "    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
       "    The most common situation would be\n",
       "    a 2D input with shape `(batch_size, input_dim)`.\n",
       "\n",
       "Output shape:\n",
       "    N-D tensor with shape: `(batch_size, ..., units)`.\n",
       "    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
       "    the output would have shape `(batch_size, units)`.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\zaid\\miniconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8f4b67a-82b2-4190-bf7f-8adeb8916b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 16)                128       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 273 (1.07 KB)\n",
      "Trainable params: 273 (1.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=5, activation='relu', input_dim=X_train.shape[1])) # 1st hidden layer( input shape is required)\n",
    "model.add(Dense(units=5, activation='tanh')) # 2nd hidden layer\n",
    "model.add(Dense(1)) # output layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a24d799a-989b-491e-b661-0c6bc4cf3c81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweighted_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mjit_compile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpss_evaluation_shards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Configures the model for training.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
       "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
       "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
       "                       tf.keras.metrics.FalseNegatives()])\n",
       "```\n",
       "\n",
       "Args:\n",
       "    optimizer: String (name of optimizer) or optimizer instance. See\n",
       "      `tf.keras.optimizers`.\n",
       "    loss: Loss function. May be a string (name of loss function), or\n",
       "      a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
       "      function is any callable with the signature `loss = fn(y_true,\n",
       "      y_pred)`, where `y_true` are the ground truth values, and\n",
       "      `y_pred` are the model's predictions.\n",
       "      `y_true` should have shape\n",
       "      `(batch_size, d0, .. dN)` (except in the case of\n",
       "      sparse loss functions such as\n",
       "      sparse categorical crossentropy which expects integer arrays of\n",
       "      shape `(batch_size, d0, .. dN-1)`).\n",
       "      `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
       "      The loss function should return a float tensor.\n",
       "      If a custom `Loss` instance is\n",
       "      used and reduction is set to `None`, return value has shape\n",
       "      `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
       "      values; otherwise, it is a scalar. If the model has multiple\n",
       "      outputs, you can use a different loss on each output by passing a\n",
       "      dictionary or a list of losses. The loss value that will be\n",
       "      minimized by the model will then be the sum of all individual\n",
       "      losses, unless `loss_weights` is specified.\n",
       "    metrics: List of metrics to be evaluated by the model during\n",
       "      training and testing. Each of this can be a string (name of a\n",
       "      built-in function), function or a `tf.keras.metrics.Metric`\n",
       "      instance. See `tf.keras.metrics`. Typically you will use\n",
       "      `metrics=['accuracy']`.\n",
       "      A function is any callable with the signature `result = fn(y_true,\n",
       "      y_pred)`. To specify different metrics for different outputs of a\n",
       "      multi-output model, you could also pass a dictionary, such as\n",
       "      `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
       "      You can also pass a list to specify a metric or a list of metrics\n",
       "      for each output, such as\n",
       "      `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
       "      or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
       "      strings 'accuracy' or 'acc', we convert this to one of\n",
       "      `tf.keras.metrics.BinaryAccuracy`,\n",
       "      `tf.keras.metrics.CategoricalAccuracy`,\n",
       "      `tf.keras.metrics.SparseCategoricalAccuracy` based on the shapes\n",
       "      of the targets and of the model output. We do a similar\n",
       "      conversion for the strings 'crossentropy' and 'ce' as well.\n",
       "      The metrics passed here are evaluated without sample weighting; if\n",
       "      you would like sample weighting to apply, you can specify your\n",
       "      metrics via the `weighted_metrics` argument instead.\n",
       "    loss_weights: Optional list or dictionary specifying scalar\n",
       "      coefficients (Python floats) to weight the loss contributions of\n",
       "      different model outputs. The loss value that will be minimized by\n",
       "      the model will then be the *weighted sum* of all individual\n",
       "      losses, weighted by the `loss_weights` coefficients.  If a list,\n",
       "      it is expected to have a 1:1 mapping to the model's outputs. If a\n",
       "      dict, it is expected to map output names (strings) to scalar\n",
       "      coefficients.\n",
       "    weighted_metrics: List of metrics to be evaluated and weighted by\n",
       "      `sample_weight` or `class_weight` during training and testing.\n",
       "    run_eagerly: Bool. If `True`, this `Model`'s logic will not be\n",
       "      wrapped in a `tf.function`. Recommended to leave this as `None`\n",
       "      unless your `Model` cannot be run inside a `tf.function`.\n",
       "      `run_eagerly=True` is not supported when using\n",
       "      `tf.distribute.experimental.ParameterServerStrategy`. Defaults to\n",
       "       `False`.\n",
       "    steps_per_execution: Int or `'auto'`. The number of batches to\n",
       "      run during each `tf.function` call. If set to \"auto\", keras will\n",
       "      automatically tune `steps_per_execution` during runtime. Running\n",
       "      multiple batches inside a single `tf.function` call can greatly\n",
       "      improve performance on TPUs, when used with distributed strategies\n",
       "      such as `ParameterServerStrategy`, or with small models with a\n",
       "      large Python overhead. At most, one full epoch will be run each\n",
       "      execution. If a number larger than the size of the epoch is\n",
       "      passed, the execution will be truncated to the size of the epoch.\n",
       "      Note that if `steps_per_execution` is set to `N`,\n",
       "      `Callback.on_batch_begin` and `Callback.on_batch_end` methods will\n",
       "      only be called every `N` batches (i.e. before/after each\n",
       "      `tf.function` execution). Defaults to `1`.\n",
       "    jit_compile: If `True`, compile the model training step with XLA.\n",
       "      [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
       "      for machine learning.\n",
       "      `jit_compile` is not enabled for by default.\n",
       "      Note that `jit_compile=True`\n",
       "      may not necessarily work for all models.\n",
       "      For more information on supported operations please refer to the\n",
       "      [XLA documentation](https://www.tensorflow.org/xla).\n",
       "      Also refer to\n",
       "      [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
       "      for more details.\n",
       "    pss_evaluation_shards: Integer or 'auto'. Used for\n",
       "      `tf.distribute.ParameterServerStrategy` training only. This arg\n",
       "      sets the number of shards to split the dataset into, to enable an\n",
       "      exact visitation guarantee for evaluation, meaning the model will\n",
       "      be applied to each dataset element exactly once, even if workers\n",
       "      fail. The dataset must be sharded to ensure separate workers do\n",
       "      not process the same data. The number of shards should be at least\n",
       "      the number of workers for good performance. A value of 'auto'\n",
       "      turns on exact evaluation and uses a heuristic for the number of\n",
       "      shards based on the number of workers. 0, meaning no\n",
       "      visitation guarantee is provided. NOTE: Custom implementations of\n",
       "      `Model.test_step` will be ignored when doing exact evaluation.\n",
       "      Defaults to `0`.\n",
       "    **kwargs: Arguments supported for backwards compatibility only.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\zaid\\miniconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f4c17fa-ca26-4e8b-a3ad-34416294dd04",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 2s 6ms/step - loss: 0.1027 - val_loss: 0.1064\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.1008 - val_loss: 0.1090\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.1072\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 0.1069\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0978 - val_loss: 0.1073\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.0979 - val_loss: 0.1126\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=5)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', )\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=8, \n",
    "                    epochs=100, \n",
    "                    validation_split=.2,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17bbfe37-ff79-4e52-a0cf-fe05f63d4b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestParams(X_train, y_train, X_test, y_test):\n",
    "    batch_sizes = [8,16,32,64]\n",
    "    epoch_list = [20, 50, 100]\n",
    "    resultsdf =[]\n",
    "    sno = 0\n",
    "    for bs in batch_sizes:\n",
    "        for epochs in epoch_list:\n",
    "            sno+=1\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=5, activation='relu', input_dim=X_train.shape[1], kernel_initializer='normal')) # 1st hidden layer( input shape is required)\n",
    "            model.add(Dense(units=5, activation='tanh', kernel_initializer='normal')) # 2nd hidden layer\n",
    "            model.add(Dense(1)) # output layer\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            model.fit(X_train, y_train, batch_size=bs, epochs=epochs, verbose=0)\n",
    "            accuracy = 100 - np.mean(100* (np.abs(y_test - model.predict(X_test))/y_test))\n",
    "            print(f'{sno} -> batch: {bs}, epochs: {epochs}, accuracy: {accuracy}')\n",
    "            resultsdf.append({\n",
    "                'sno': sno,\n",
    "                'bs': bs,\n",
    "                'epochs':epochs,\n",
    "                'acc': accuracy\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e05500e8-73d4-4ac2-9f70-9f072adefa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 1ms/step\n",
      "1 -> batch: 8, epochs: 20, accuracy: 66.72412005749095\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "2 -> batch: 8, epochs: 50, accuracy: 68.23634100899719\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "3 -> batch: 8, epochs: 100, accuracy: 73.49885053483656\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "4 -> batch: 16, epochs: 20, accuracy: 65.91162747765748\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "5 -> batch: 16, epochs: 50, accuracy: 65.00777014626647\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "6 -> batch: 16, epochs: 100, accuracy: 66.43275375633189\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "7 -> batch: 32, epochs: 20, accuracy: 64.63183048598059\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "8 -> batch: 32, epochs: 50, accuracy: 65.55419242800885\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9 -> batch: 32, epochs: 100, accuracy: 65.45303054532123\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "10 -> batch: 64, epochs: 20, accuracy: 64.44450938595884\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "11 -> batch: 64, epochs: 50, accuracy: 65.58540323370705\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "12 -> batch: 64, epochs: 100, accuracy: 68.1275501177196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sno</th>\n",
       "      <th>bs</th>\n",
       "      <th>epochs</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>66.724120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>68.236341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>73.498851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>65.911627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>65.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>66.432754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>64.631830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>65.554192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>65.453031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>64.444509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>50</td>\n",
       "      <td>65.585403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>68.127550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sno  bs  epochs        acc\n",
       "0     1   8      20  66.724120\n",
       "1     2   8      50  68.236341\n",
       "2     3   8     100  73.498851\n",
       "3     4  16      20  65.911627\n",
       "4     5  16      50  65.007770\n",
       "5     6  16     100  66.432754\n",
       "6     7  32      20  64.631830\n",
       "7     8  32      50  65.554192\n",
       "8     9  32     100  65.453031\n",
       "9    10  64      20  64.444509\n",
       "10   11  64      50  65.585403\n",
       "11   12  64     100  68.127550"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96ce7f-b67a-4cd5-9ca3-149a252e2341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
